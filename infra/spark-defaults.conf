# === Iceberg catalog (Nessie) ===
spark.sql.catalog.nessie org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.nessie.catalog-impl org.apache.iceberg.nessie.NessieCatalog
spark.sql.catalog.nessie.uri http://nessie:19120/api/v1
spark.sql.catalog.nessie.ref main
spark.sql.catalog.nessie.warehouse s3a://lotus-lakehouse

# === MinIO / S3 endpoint ===
spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
spark.hadoop.fs.s3a.endpoint http://datalake-storage:9000
spark.hadoop.fs.s3a.access.key ${AWS_ACCESS_KEY_ID}
spark.hadoop.fs.s3a.secret.key ${AWS_SECRET_ACCESS_KEY}
spark.hadoop.fs.s3a.path.style.access true
spark.hadoop.fs.s3a.connection.ssl.enabled false

# === Propagate env vars into Python executors ===
spark.executorEnv.ICEBERG_CATALOG_URI=http://nessie:19120/api/v1
spark.executorEnv.ICEBERG_CATALOG_WAREHOUSE=s3a://lotus-lakehouse
spark.executorEnv.S3_ENDPOINT=http://datalake-storage:9000
spark.executorEnv.AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
spark.executorEnv.AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
spark.executorEnv.ICEBERG_CATALOG_REF=main
spark.executorEnv.ICEBERG_CATALOG_AUTH=NONE

# === Resource tuning ===
spark.executor.memory 2g
spark.executor.cores 2
spark.driver.memory 1g